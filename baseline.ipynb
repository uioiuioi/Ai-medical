{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm\nimport sys\nsys.path.append('../input/ttach-kaggle')\nimport ttach","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:21:38.669615Z","iopub.execute_input":"2022-03-21T03:21:38.670197Z","iopub.status.idle":"2022-03-21T03:21:49.833756Z","shell.execute_reply.started":"2022-03-21T03:21:38.670161Z","shell.execute_reply":"2022-03-21T03:21:49.832891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ライブラリ読み込み\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport glob\nimport pickle\nimport time\nimport gc\nimport random\nimport copy\nimport math\nfrom math import ceil\nimport sys\nimport cv2\nimport pandas_profiling as pdp\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport ttach as tta\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom collections import defaultdict\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A\nimport timm\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 100) # 表示できる表の列数","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-21T03:21:49.835548Z","iopub.execute_input":"2022-03-21T03:21:49.835812Z","iopub.status.idle":"2022-03-21T03:21:53.700392Z","shell.execute_reply.started":"2022-03-21T03:21:49.835781Z","shell.execute_reply":"2022-03-21T03:21:53.699525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timm.list_models(pretrained=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:21:53.702497Z","iopub.execute_input":"2022-03-21T03:21:53.703043Z","iopub.status.idle":"2022-03-21T03:21:53.727045Z","shell.execute_reply.started":"2022-03-21T03:21:53.703002Z","shell.execute_reply":"2022-03-21T03:21:53.725877Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Config = {\"seed\": 14,\n          \"epochs\": 300,\n          \"img_size\": 256,\n          \"batch_size\": 32,\n          \"learning_rate\": 0.001,\n          \"scheduler\": 'OneCycleLR',\n          \"min_lr\": 1e-6,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"model_name\": \"resnet18d\",\n          \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n          }","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:42.330209Z","iopub.execute_input":"2022-03-21T03:53:42.33073Z","iopub.status.idle":"2022-03-21T03:53:42.335661Z","shell.execute_reply.started":"2022-03-21T03:53:42.330675Z","shell.execute_reply":"2022-03-21T03:53:42.334993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(Config['seed'])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:42.749023Z","iopub.execute_input":"2022-03-21T03:53:42.749253Z","iopub.status.idle":"2022-03-21T03:53:42.755146Z","shell.execute_reply.started":"2022-03-21T03:53:42.749224Z","shell.execute_reply":"2022-03-21T03:53:42.754161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"../input/ai-medical-contest-2022/\"\ndf_train = pd.read_csv(DATA_DIR + \"train.csv\")\ndf_test = pd.read_csv(DATA_DIR + \"test.csv\")\ndf_unlabeled = pd.read_csv(DATA_DIR + \"unlabeled.csv\")\ndf_sub = pd.read_csv(DATA_DIR + \"sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:43.027281Z","iopub.execute_input":"2022-03-21T03:53:43.027538Z","iopub.status.idle":"2022-03-21T03:53:43.073204Z","shell.execute_reply.started":"2022-03-21T03:53:43.02751Z","shell.execute_reply":"2022-03-21T03:53:43.072556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 画像データのpathの列を追加.\ndf_train['path'] = df_train['id'].apply(lambda x: \"../input/ai-medical-contest-2022/image/image/{}.png\".format(x))\ndf_test['path'] = df_test['id'].apply(lambda x: \"../input/ai-medical-contest-2022/image/image/{}.png\".format(x))\ndf_unlabeled['path'] = df_unlabeled['id'].apply(lambda x: \"../input/ai-medical-contest-2022/image/image/{}.png\".format(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:43.265488Z","iopub.execute_input":"2022-03-21T03:53:43.265833Z","iopub.status.idle":"2022-03-21T03:53:43.286931Z","shell.execute_reply.started":"2022-03-21T03:53:43.2658Z","shell.execute_reply":"2022-03-21T03:53:43.286297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=Config['n_fold'], random_state=Config['seed'], shuffle=True)\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train['pneumonia'])):\n      df_train.loc[val_ , \"kfold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:43.550636Z","iopub.execute_input":"2022-03-21T03:53:43.550855Z","iopub.status.idle":"2022-03-21T03:53:43.563341Z","shell.execute_reply.started":"2022-03-21T03:53:43.550829Z","shell.execute_reply":"2022-03-21T03:53:43.562581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['path'].values\n        self.labels = df['pneumonia'].values\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.labels[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return {\n            'image': img,\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:44.180191Z","iopub.execute_input":"2022-03-21T03:53:44.180678Z","iopub.status.idle":"2022-03-21T03:53:44.18811Z","shell.execute_reply.started":"2022-03-21T03:53:44.180644Z","shell.execute_reply":"2022-03-21T03:53:44.187385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(Config['img_size'], Config['img_size']),\n        A.HorizontalFlip(p=1.0),\n        A.VerticalFlip(p=1.0),\n        A.Rotate(limit=180, p=0.7),\n        A.ShiftScaleRotate(\n            shift_limit = 0.1, scale_limit=0.1, rotate_limit=45, p=0.5\n        ),\n#         A.HueSaturationValue(\n#             hue_shift_limit=0.2, sat_shift_limit=0.2,\n#             val_shift_limit=0.2, p=0.5\n#         ),\n#         A.RandomBrightnessContrast(\n#         brightness_limit=(-0.1, 0.1),\n#             contrast_limit=(-0.1, 0.1), p=0.5\n#         ),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(Config['img_size'], Config['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:44.643874Z","iopub.execute_input":"2022-03-21T03:53:44.644416Z","iopub.status.idle":"2022-03-21T03:53:44.652672Z","shell.execute_reply.started":"2022-03-21T03:53:44.644376Z","shell.execute_reply":"2022-03-21T03:53:44.651993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class block(nn.Module):\n    def __init__(self, first_conv_in_channels, first_conv_out_channels, identity_conv=None, stride=1):\n        \"\"\"\n        残差ブロックを作成するクラス\n        Args:\n            first_conv_in_channels : 1番目のconv層（1×1）のinput channel数\n            first_conv_out_channels : 1番目のconv層（1×1）のoutput channel数\n            identity_conv : channel数調整用のconv層\n            stride : 3×3conv層におけるstide数。sizeを半分にしたいときは2に設定\n        \"\"\"        \n        super(block, self).__init__()\n\n        # 1番目のconv層（1×1）\n        self.conv1 = nn.Conv2d(\n            first_conv_in_channels, first_conv_out_channels, kernel_size=1, stride=1, padding=0)\n        self.bn1 = nn.BatchNorm2d(first_conv_out_channels)\n\n        # 2番目のconv層（3×3）\n        # パターン3の時はsizeを変更できるようにstrideは可変\n        self.conv2 = nn.Conv2d(\n            first_conv_out_channels, first_conv_out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn2 = nn.BatchNorm2d(first_conv_out_channels)\n\n        # 3番目のconv層（1×1）\n        # output channelはinput channelの4倍になる\n        self.conv3 = nn.Conv2d(\n            first_conv_out_channels, first_conv_out_channels*4, kernel_size=1, stride=1, padding=0)\n        self.bn3 = nn.BatchNorm2d(first_conv_out_channels*4)\n        self.relu = nn.ReLU()\n\n        # identityのchannel数の調整が必要な場合はconv層（1×1）を用意、不要な場合はNone\n        self.identity_conv = identity_conv\n\n    def forward(self, x):\n\n        identity = x.clone()  # 入力を保持する\n\n        x = self.conv1(x)  # 1×1の畳み込み\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)  # 3×3の畳み込み（パターン3の時はstrideが2になるため、ここでsizeが半分になる）\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(x)  # 1×1の畳み込み\n        x = self.bn3(x)\n\n        # 必要な場合はconv層（1×1）を通してidentityのchannel数の調整してから足す\n        if self.identity_conv is not None:\n            identity = self.identity_conv(identity)\n        x += identity\n\n        x = self.relu(x)\n\n        return x\n    \n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_classes):\n        super(ResNet, self).__init__()\n\n        # conv1はアーキテクチャ通りにベタ打ち\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # conv2_xはサイズの変更は不要のため、strideは1\n        self.conv2_x = self._make_layer(block, 3, res_block_in_channels=64, first_conv_out_channels=64, stride=1)\n\n        # conv3_x以降はサイズの変更をする必要があるため、strideは2\n        self.conv3_x = self._make_layer(block, 4, res_block_in_channels=256,  first_conv_out_channels=128, stride=2)\n        self.conv4_x = self._make_layer(block, 6, res_block_in_channels=512,  first_conv_out_channels=256, stride=2)\n        self.conv5_x = self._make_layer(block, 3, res_block_in_channels=1024, first_conv_out_channels=512, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc1 = nn.Linear(512*4, num_classes)\n#         self.fc2 = nn.Linear(512*3, 512*2)\n#         self.fc3 = nn.Linear(512*2, 512)\n#         self.fc4 = nn.Linear(512, num_classes)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self,x):\n\n        x = self.conv1(x)   # in:(3,224*224)、out:(64,112*112)\n        x = self.bn1(x)     # in:(64,112*112)、out:(64,112*112)\n        x = self.relu(x)    # in:(64,112*112)、out:(64,112*112)\n        x = self.maxpool(x) # in:(64,112*112)、out:(64,56*56)\n\n        x = self.conv2_x(x)  # in:(64,56*56)  、out:(256,56*56)\n        x = self.conv3_x(x)  # in:(256,56*56) 、out:(512,28*28)\n        x = self.conv4_x(x)  # in:(512,28*28) 、out:(1024,14*14)\n        x = self.conv5_x(x)  # in:(1024,14*14)、out:(2048,7*7)\n        x = self.avgpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc1(x)\n#         x = self.fc2(x)\n#         x = self.fc3(x)\n#         x = self.fc4(x)\n        x = self.sigmoid(x)\n\n        return x\n\n    def _make_layer(self, block, num_res_blocks, res_block_in_channels, first_conv_out_channels, stride):\n        layers = []\n\n        # 1つ目の残差ブロックではchannel調整、及びsize調整が発生する\n        # identifyを足す前に1×1のconv層を追加し、サイズ調整が必要な場合はstrideを2に設定\n        identity_conv = nn.Conv2d(res_block_in_channels, first_conv_out_channels*4, kernel_size=1,stride=stride)\n        layers.append(block(res_block_in_channels, first_conv_out_channels, identity_conv, stride))\n\n        # 2つ目以降のinput_channel数は1つ目のoutput_channelの4倍\n        in_channels = first_conv_out_channels*4\n\n        # channel調整、size調整は発生しないため、identity_convはNone、strideは1\n        for i in range(num_res_blocks - 1):\n            layers.append(block(in_channels, first_conv_out_channels, identity_conv=None, stride=1))\n\n        return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:45.420774Z","iopub.execute_input":"2022-03-21T03:53:45.421242Z","iopub.status.idle":"2022-03-21T03:53:45.443478Z","shell.execute_reply.started":"2022-03-21T03:53:45.421203Z","shell.execute_reply":"2022-03-21T03:53:45.442661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch, fold):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.float)\n        labels = labels.unsqueeze(-1)\n        \n        batch_size = Config['batch_size']\n        optimizer.zero_grad()\n        outputs = model(images)\n        sigmoid = nn.Sigmoid()\n        outputs = sigmoid(outputs)\n        criterion = nn.BCELoss()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n        \n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:46.188339Z","iopub.execute_input":"2022-03-21T03:53:46.189049Z","iopub.status.idle":"2022-03-21T03:53:46.197908Z","shell.execute_reply.started":"2022-03-21T03:53:46.189011Z","shell.execute_reply":"2022-03-21T03:53:46.197143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.inference_mode()\ndef valid_one_epoch(model, dataloader, device, epoch, fold):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.float)\n        labels = labels.unsqueeze(-1)\n\n        batch_size = Config['batch_size']\n        outputs = model(images)\n        sigmoid = nn.Sigmoid()\n        outputs = sigmoid(outputs)\n        criterion = nn.BCELoss()\n        loss = criterion(outputs, labels)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n\n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:48.551277Z","iopub.execute_input":"2022-03-21T03:53:48.551956Z","iopub.status.idle":"2022-03-21T03:53:48.564839Z","shell.execute_reply.started":"2022-03-21T03:53:48.551906Z","shell.execute_reply":"2022-03-21T03:53:48.564005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler,\n                                           dataloader=train_loader, \n                                           device=Config['device'], epoch=epoch, fold=fold)\n        \n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=Config['device'], \n                                         epoch=epoch, fold=fold)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        # deep copy the model\n        if val_epoch_loss <= best_epoch_loss:\n            es = 0\n            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"{Config['model_name']}fold{fold}best.pt\"\n            torch.save(model.state_dict(), PATH)\n        else:\n            es += 1\n            if es > 9:\n                print(\"Early stopping with best_acc: \", best_epoch_loss)\n                break\n\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    model.load_state_dict(best_model_wts)\n        \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:53:59.848056Z","iopub.execute_input":"2022-03-21T03:53:59.848333Z","iopub.status.idle":"2022-03-21T03:53:59.860744Z","shell.execute_reply.started":"2022-03-21T03:53:59.848287Z","shell.execute_reply":"2022-03-21T03:53:59.859837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:54:00.249751Z","iopub.execute_input":"2022-03-21T03:54:00.250022Z","iopub.status.idle":"2022-03-21T03:54:00.255002Z","shell.execute_reply.started":"2022-03-21T03:54:00.249991Z","shell.execute_reply":"2022-03-21T03:54:00.254267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_loaders(df, fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n        \n    train_dataset = MyDataset(df_train, transforms=data_transforms[\"train\"])\n    valid_dataset = MyDataset(df_valid, transforms=data_transforms[\"valid\"])\n\n    train_loader = DataLoader(train_dataset, batch_size=Config['batch_size'], \n                              num_workers=0, shuffle=True, pin_memory=True, drop_last=True, worker_init_fn=seed_worker)\n    valid_loader = DataLoader(valid_dataset, batch_size=Config['batch_size'], \n                              num_workers=0, shuffle=False, pin_memory=True, worker_init_fn=seed_worker)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:54:00.532368Z","iopub.execute_input":"2022-03-21T03:54:00.532924Z","iopub.status.idle":"2022-03-21T03:54:00.539203Z","shell.execute_reply.started":"2022-03-21T03:54:00.532888Z","shell.execute_reply":"2022-03-21T03:54:00.538498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(Config['n_fold']):\n#     model = ResNet(block, 1)\n    model = timm.create_model(Config['model_name'], pretrained=False, num_classes=1)\n    model.to(Config['device'])\n    train_loader, valid_loader = prepare_loaders(df_train, fold=i)\n    optimizer = optim.Adam(model.parameters(), lr=Config['learning_rate'], weight_decay=Config['weight_decay'])\n    scheduler = lr_scheduler.OneCycleLR(optimizer,max_lr =Config['learning_rate'],total_steps = Config['epochs'] * len(train_loader))\n    model, history = run_training(model, optimizer, scheduler, device=Config['device'], num_epochs=Config['epochs'], fold=i)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:54:09.598906Z","iopub.execute_input":"2022-03-21T03:54:09.599649Z","iopub.status.idle":"2022-03-21T04:10:08.71125Z","shell.execute_reply.started":"2022-03-21T03:54:09.599605Z","shell.execute_reply":"2022-03-21T04:10:08.710276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = tta.Compose(\n    [\n        tta.HorizontalFlip(),\n        tta.VerticalFlip(),\n        tta.Rotate90(angles=[0, 90, 180]),\n#         tta.Scale(scales=[1, 2, 4]),\n#         tta.Multiply(factors=[0.9, 1, 1.1]),        \n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T02:32:22.415569Z","iopub.execute_input":"2022-03-21T02:32:22.416062Z","iopub.status.idle":"2022-03-21T02:32:22.42289Z","shell.execute_reply.started":"2022-03-21T02:32:22.416019Z","shell.execute_reply":"2022-03-21T02:32:22.421337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(test_dataloader, model):\n    \n    allpreds=[]\n\n    total_loss = 0 # Initializing total loss\n\n    model.eval()\n\n    for a in test_dataloader:\n\n        with torch.no_grad():\n            images = a[\"image\"].to(Config['device'])\n\n            output = model(images) # prediction\n            sigmoid = nn.Sigmoid()\n            output = sigmoid(output)\n            allpreds.append(output.detach().cpu().numpy())\n\n    allpreds = np.concatenate(allpreds)\n    \n    return allpreds","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:51:10.729527Z","iopub.execute_input":"2022-03-21T03:51:10.729787Z","iopub.status.idle":"2022-03-21T03:51:10.735483Z","shell.execute_reply.started":"2022-03-21T03:51:10.729758Z","shell.execute_reply":"2022-03-21T03:51:10.734801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['pneumonia'] = 0.5\ntest_dataset = MyDataset(df_test, transforms=data_transforms[\"valid\"])\ntest_loader = DataLoader(test_dataset, batch_size=Config['batch_size'], \n                          num_workers=2, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:51:12.229959Z","iopub.execute_input":"2022-03-21T03:51:12.230239Z","iopub.status.idle":"2022-03-21T03:51:12.235963Z","shell.execute_reply.started":"2022-03-21T03:51:12.230208Z","shell.execute_reply":"2022-03-21T03:51:12.235308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:51:12.901749Z","iopub.execute_input":"2022-03-21T03:51:12.902227Z","iopub.status.idle":"2022-03-21T03:51:12.906252Z","shell.execute_reply.started":"2022-03-21T03:51:12.902189Z","shell.execute_reply":"2022-03-21T03:51:12.90539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(Config['n_fold']):\n    #     model = ResNet(block, 1)\n    model = timm.create_model(Config['model_name'], pretrained=True, num_classes=1)\n    model_name = f'./resnet18dfold{i}best.pt'\n    model.load_state_dict(torch.load(model_name))\n    model.to(Config['device'])\n#     tta_model = tta.ClassificationTTAWrapper(model, transforms)\n#     tta_model.to(Config['device'])\n    pred = inference(test_loader, model)\n    preds.append(pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:51:50.422049Z","iopub.execute_input":"2022-03-21T03:51:50.422348Z","iopub.status.idle":"2022-03-21T03:52:12.961228Z","shell.execute_reply.started":"2022-03-21T03:51:50.422299Z","shell.execute_reply":"2022-03-21T03:52:12.960265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.array(preds)\naverage_preds = np.mean(preds, axis=0)\ndf_sub['pneumonia'] = average_preds\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T03:52:12.963367Z","iopub.execute_input":"2022-03-21T03:52:12.96366Z","iopub.status.idle":"2022-03-21T03:52:12.976919Z","shell.execute_reply.started":"2022-03-21T03:52:12.963612Z","shell.execute_reply":"2022-03-21T03:52:12.97607Z"},"trusted":true},"execution_count":null,"outputs":[]}]}